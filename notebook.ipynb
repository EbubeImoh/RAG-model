{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcd633d",
   "metadata": {},
   "source": [
    "# RAG System Test Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72eb811",
   "metadata": {},
   "source": [
    "### Load & Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST IS TO LOAD AND PROCESS THE DOCUMENT\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader # type: ignore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "\n",
    "loader = PyPDFLoader(\"knowledge-base/Company Profile.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6488ebfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b2656",
   "metadata": {},
   "source": [
    "### Embed Chunks + Store in Vector DB (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b05991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/3wx8_rg50x59zf8fz_nvhrq80000gn/T/ipykernel_47421/2509144484.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "/Users/user/RAG model/RAG-model/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# NEXT IS TO EMBED CHUNKS AND STORE IN VECTOR DB(FAISS)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "vectorstore.save_local(\"my_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cacf36c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12fdf1060>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166eb54",
   "metadata": {},
   "source": [
    "### Querying (RAG Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd3c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, here's the answer to the question \"What are the company core values?\" The company's core values are:\n",
      "\n",
      "1. Integrity - We uphold transparency, honesty, and ethical practices in all our dealings.\n",
      "2. Innovation - We continuously develop and deploy forward-thinking financial technologies.\n",
      "3. Customer-Centricity - We prioritize our clients' needs and success in driving impact.\n",
      "4. Security - We invest heavily in the protection of our clients' data and transaction records.\n",
      "5. Collaboration - We foster strong partnerships internally and externally to drive impact.\n",
      "6. Diversification and Inclusion - We strive for a diverse and inclusive culture that values global diversity.\n",
      "7. Simplifying cross-border payment solutions through advanced technology, regulatory compliance, and exceptional customer support. \n",
      "8. Advance technologies and regulatory adherence to ensure the company's tech stack is scalable and resilient. The company also coordinates with legal teams to remain compliant with global standards.\n"
     ]
    }
   ],
   "source": [
    "# NEXT IS TO QUERY THE RAG MODEL(TEST WITH A SIMPLE QUERY)\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"tinyllama\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
    "response = qa_chain.run(\"What are the company core values?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0f7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Time: 0.43 sec\n",
      "Generation Time: 122.14 sec\n",
      "\n",
      "Answer: The question asks for the name of a company. The correct answer is GlobalPay Financial Services, as stated in the given context.\n"
     ]
    }
   ],
   "source": [
    "#CHECKING THE TIME TAKEN FOR RETRIEVAL AND GENERATION\n",
    "import time\n",
    "\n",
    "query = \"What is the name of the company?\"\n",
    "\n",
    "start = time.time()\n",
    "retrieved_docs = vectorstore.similarity_search(query)\n",
    "print(f\"Retrieval Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "start = time.time()\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Generation Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "print(\"\\nAnswer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158f48e",
   "metadata": {},
   "source": [
    "### UI with Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bc2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:26:56.242 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.847 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/user/RAG model/RAG-model/.venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-15 17:26:56.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.849 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.852 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.864 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-15 17:26:56.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 17:26:56.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Free RAG Assistant\")\n",
    "query = st.text_input(\"Ask a question:\")\n",
    "if query:\n",
    "    result = qa_chain.run(query)\n",
    "    st.write(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c02865",
   "metadata": {},
   "source": [
    "### Logging Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGGING THE QUERIES\n",
    "import logging\n",
    "logging.basicConfig(filename='queries.log', level=logging.INFO)\n",
    "logging.info(f\"User asked: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
