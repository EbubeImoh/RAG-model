{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcd633d",
   "metadata": {},
   "source": [
    "# RAG System Test Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72eb811",
   "metadata": {},
   "source": [
    "### Load & Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST IS TO LOAD AND PROCESS THE DOCUMENT\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader # type: ignore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "\n",
    "loader = PyPDFLoader(\"knowledge-base/Company Profile.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b2656",
   "metadata": {},
   "source": [
    "### Embed Chunks + Store in Vector DB (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b05991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT IS TO EMBED CHUNKS AND STORE IN VECTOR DB(FAISS)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "vectorstore.save_local(\"my_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166eb54",
   "metadata": {},
   "source": [
    "### Querying (RAG Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"How can i help you?\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b8c98",
   "metadata": {},
   "source": [
    "### Query Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81082285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23741f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Use accelerator to place the model and inputs on the available device\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hello, world!\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the appropriate device\n",
    "inputs = {key: value.to(accelerator.device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e019f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_paraphrase(input_text: str, n_variants=3, max_length=100):\n",
    "    # Create multiple paraphrasing prompts\n",
    "    prompts = [\n",
    "        f\"Paraphrase the following sentence differently: {input_text}\",\n",
    "        f\"Rewrite this in a new way: {input_text}\",\n",
    "        f\"Say this with different words: {input_text}\",\n",
    "        f\"Change the wording while keeping the same meaning: {input_text}\",\n",
    "    ][:n_variants]\n",
    "\n",
    "    # Generate outputs for each prompt\n",
    "    outputs = []\n",
    "    for prompt in prompts:\n",
    "        result = generator(prompt, max_length=max_length, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "        text = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
    "        outputs.append(text)\n",
    "\n",
    "    return list(set(outputs))  # Return unique outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrases = multi_query_paraphrase(query, n_variants=3)\n",
    "for i, p in enumerate(paraphrases, 1):\n",
    "    print(f\"{i}. {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e134a",
   "metadata": {},
   "source": [
    "### Routing(Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# routing_labels.py\n",
    "\n",
    "routing_labels = {\n",
    "    \"database\": \"Questions about internal records, metrics, tables, or structured data.\",\n",
    "    \"vectorstore\": \"Questions related to uploaded documents, PDFs, knowledge bases, or internal wikis.\",\n",
    "    \"web\": \"Questions that might require up-to-date, external information not present in the system.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf550de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def route_question(question: str, threshold: float = 0.4):\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    similarities = {}\n",
    "    for label, description in routing_labels.items():\n",
    "        desc_embedding = model.encode(description, convert_to_tensor=True)\n",
    "        sim_score = float(util.pytorch_cos_sim(question_embedding, desc_embedding))\n",
    "        similarities[label] = sim_score\n",
    "\n",
    "    # Determine best route\n",
    "    best_route = max(similarities, key=similarities.get)\n",
    "    best_score = similarities[best_route]\n",
    "\n",
    "    # If below threshold, fallback to web\n",
    "    if best_score < threshold:\n",
    "        return \"web\", similarities\n",
    "\n",
    "    return best_route, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3dffc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock response functions\n",
    "def query_database(question):\n",
    "    return f\"[DB Answer] to: {question}\"\n",
    "\n",
    "def query_vectorstore(question):\n",
    "    return f\"[Vector RAG Answer] to: {question}\"\n",
    "\n",
    "def query_web(question):\n",
    "    return f\"[Web Search Answer] to: {question}\"\n",
    "\n",
    "\n",
    "def handle_question(user_question):\n",
    "    route, sim_scores = route_question(user_question)\n",
    "\n",
    "    print(f\"\\nRouting to: {route}\")\n",
    "    print(f\"Similarities: { {k: round(v, 3) for k, v in sim_scores.items()} }\")\n",
    "\n",
    "    if route == \"database\":\n",
    "        return query_database(user_question)\n",
    "    elif route == \"vectorstore\":\n",
    "        return query_vectorstore(user_question)\n",
    "    else:\n",
    "        return query_web(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15021e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI interface\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nAsk a question: \")\n",
    "            if question.lower() in {\"exit\", \"quit\"}:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            answer = handle_question(question)\n",
    "            print(answer)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user. Exiting.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63970625",
   "metadata": {},
   "source": [
    "### Query Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de86642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b249121d",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT IS TO QUERY THE RAG MODEL(TEST WITH A SIMPLE QUERY)\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"tinyllama\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE TIME TAKEN FOR RETRIEVAL AND GENERATION\n",
    "import time\n",
    "\n",
    "query = \"What is the name of the company?\"\n",
    "\n",
    "start = time.time()\n",
    "retrieved_docs = vectorstore.similarity_search(query)\n",
    "print(f\"Retrieval Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "start = time.time()\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Generation Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "print(\"\\nAnswer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158f48e",
   "metadata": {},
   "source": [
    "### UI with Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"Free RAG Assistant\")\n",
    "query = st.text_input(\"Ask a question:\")\n",
    "if query:\n",
    "    result = qa_chain.run(query)\n",
    "    st.write(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c02865",
   "metadata": {},
   "source": [
    "### Logging Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGGING THE QUERIES\n",
    "import logging\n",
    "logging.basicConfig(filename='queries.log', level=logging.INFO)\n",
    "logging.info(f\"User asked: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
